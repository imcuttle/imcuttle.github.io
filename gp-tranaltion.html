<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/png" href="/favicon.png" />
    <title> imCuttle </title>
    <link rel="stylesheet" href="/style.css">
</head>
<body>
<div id="root">
    <div class="wrap" data-reactroot="" data-reactid="1" data-react-checksum="-112362393"><header data-reactid="2"><a class="logo-link" href="/" data-reactid="3"><img src="/favicon.png" data-reactid="4"/></a><ul class="nav nav-list" data-reactid="5"><li class="nav-list-item" data-reactid="6"><a class="nav-list-link" href="/posts/1" data-reactid="7">Posts</a></li></ul></header><main data-reactid="8"><div class="post" data-reactid="9"><article class="post-block" data-reactid="10"><h1 class="post-title" data-reactid="11">基于特征脸的人脸识别</h1><div class="post-info" data-reactid="12"><time datetime="2017-04-19T17:10:28+00:00" data-reactid="13">Apr 19, 2017 5:10 PM</time></div></article><div class="post-content" data-reactid="14"><article data-reactid="15"><p data-reactid="16"><!-- react-text: 17 -->原文: <!-- /react-text --><a href="http://openbio.sourceforge.net/resources/eigenfaces/eigenfaces.pdf" data-reactid="18">http://openbio.sourceforge.net/resources/eigenfaces/eigenfaces.pdf</a><br data-reactid="19"/><!-- react-text: 20 -->
作者：Dimitri PISSARENKO
时间：2002年12月1日<!-- /react-text --></p><!-- react-text: 21 -->
<!-- /react-text --><h2 id="总目" data-reactid="22"><a href="#%E6%80%BB%E7%9B%AE" aria-hidden="true" data-reactid="23"><span class="icon icon-link" data-reactid="24"></span></a><!-- react-text: 25 -->总目<!-- /react-text --></h2><!-- react-text: 26 -->
<!-- /react-text --><p data-reactid="27">该记录是基于 M. Turk and A. Pentland（1991b）、 M. Turk and A. Pentland（1991a）和 Smith（2002）文献所作。</p><!-- react-text: 28 -->
<!-- /react-text --><h2 id="特征脸是怎么工作的？" data-reactid="29"><a href="#%E7%89%B9%E5%BE%81%E8%84%B8%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84%EF%BC%9F" aria-hidden="true" data-reactid="30"><span class="icon icon-link" data-reactid="31"></span></a><!-- react-text: 32 -->特征脸是怎么工作的？<!-- /react-text --></h2><!-- react-text: 33 -->
<!-- /react-text --><p data-reactid="34"><!-- react-text: 35 -->人脸识别的任务本质就是：区别开一些输入信号（图像数据），将其划分进一些分类（人脸）中。<!-- /react-text --><br data-reactid="36"/><!-- react-text: 37 -->
输入信号高度噪声（例如噪声是由不同引起的：
照明条件，姿势等），但输入图像不是完全随机的
尽管它们有差异，但是在任何输入信号中都存在模式。这样
在所有信号中可以观察到的模式可能是在面部识别领域。
任何面孔以及亲属中存在某些物体（眼睛，鼻子，嘴巴）
这些物体之间的距离。这些特征被称为特征脸
面部识别领域（或主要组成部分）。它们可以通过名为PCA（主成分分析法）的数学方法从原始图像数据中提取出来。
通过 PCA 可以能够将训练集的每个原图转化为相关的特征脸。PCA 一个重要的特征是能够组合特征脸来重构训练集中的原图。<!-- /react-text --><br data-reactid="38"/><!-- react-text: 39 -->
记住特征脸不仅仅是面部的特征。所以说如果将每个特征脸按照正确的比例相加，原始脸部图像可以从特征脸重构出来。每个特征脸代表脸部都某些特征，其可能存在或不存在于原图中。如果特征以较高程度表现在原图中，则该特征对应的特征脸应该在特征脸集合相加的总和中，占用更大的比例。相反，特定的特征在原始图像中不存在（或几乎不存在），然后相应的特征脸应该贡献一个较小的（或根本不是）部分的总和。<!-- /react-text --><br data-reactid="40"/><!-- react-text: 41 -->
因此，为了从特征脸重构原始图像，需要得到一系列特征脸的权重，也就是说，重构的原图图像等于所有特征面的总和，每个特征脸都有明确的权重。这个权重表示了指定特征（特征脸）在原图中所占的程度。<!-- /react-text --></p><!-- react-text: 42 -->
<!-- /react-text --><p data-reactid="43">如果使用从原始图像提取的所有特征脸，可以重构来自特征脸的原始图像。但也可以只使用一部分特征脸，重建的图像是原始图像的近似值。然而，这样可以确保由于省略某些特征脸而造成的损失最小化，选择最重要的特征（特征脸）。</p><!-- react-text: 44 -->
<!-- /react-text --><p data-reactid="45"><!-- react-text: 46 -->由于计算资源的匮乏，特征脸的部分选择（降维）是必要的。那这与人脸识别有什么关系呢？不仅可能从给定的一组权重的特征脸得到面部，但也可以用相反的方式，从特征脸和原人脸面部得到一组权重。使用这个权重可以确定两件重要的事情：
1. 确定所讨论的图像是否是一张脸。<!-- /react-text --><br data-reactid="47"/><!-- react-text: 48 -->
输入图像的权重与人脸图像（我们知道是人脸）的权重相差太大，则该输入不是人脸。
2. 相似的脸（图像）具有相似的特征（特征脸）权重。
可以从所有可用的图像中提取权重，通过权重可以进行分组到集群。也就是说，具有相似权重的所有图像可能是类似的面孔。<!-- /react-text --></p><!-- react-text: 49 -->
<!-- /react-text --><h2 id="算法概述" data-reactid="50"><a href="#%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0" aria-hidden="true" data-reactid="51"><span class="icon icon-link" data-reactid="52"></span></a><!-- react-text: 53 -->算法概述<!-- /react-text --></h2><!-- react-text: 54 -->
<!-- /react-text --><ol data-reactid="55"><!-- react-text: 56 -->
<!-- /react-text --><li data-reactid="57">首先，将训练集的原始图像转换为一组特征脸 E。</li><!-- react-text: 58 -->
<!-- /react-text --><li data-reactid="59">然后，计算每个图像在 E 上的一组权重，保存在 W。</li><!-- react-text: 60 -->
<!-- /react-text --></ol><!-- react-text: 61 -->
<!-- /react-text --><p data-reactid="62">观察未知图像 X，计算取特征权重，存储在向量 Wx 中。之后，将Wx与知道他们是面孔（训练的权重W）的其他权重进行比较。一种方法是将每个权重向量视为空间中的一个点计算来自WX的权重向量与权重之间的平均距离D.未知图像的矢量WX（附录A中描述的欧几里德距离）。如果该平均距离超过某个阈值θ，那么未知图像WX的权重向量也与权重“分开”的脸。在这种情况下，未知X被认为不是脸。否则（如果X实际上是一个脸），它的权重向量WX被存储以备以后分类。最优经验性地确定阈值θ。</p><!-- react-text: 63 -->
<!-- /react-text --><h2 id="特征向量和特征值" data-reactid="64"><a href="#%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E5%92%8C%E7%89%B9%E5%BE%81%E5%80%BC" aria-hidden="true" data-reactid="65"><span class="icon icon-link" data-reactid="66"></span></a><!-- react-text: 67 -->特征向量和特征值<!-- /react-text --></h2><!-- react-text: 68 -->
<!-- /react-text --><p data-reactid="69">矩阵的特征向量是一个向量，其与矩阵相乘，则结果总为该向量的整数倍。这个整数值是相应的特征向量的特征值。这种关系可以用公式 M ×
u = λ × u 来描述，其中u是矩阵M的特征向量，λ是相应的特征值。 特征向量具有以下属性：</p><!-- react-text: 70 -->
<!-- /react-text --><ul data-reactid="71"><!-- react-text: 72 -->
<!-- /react-text --><li data-reactid="73">它们只能用于方阵</li><!-- react-text: 74 -->
<!-- /react-text --><li data-reactid="75">n×n矩阵中有n个特征向量（和相应的特征值）</li><!-- react-text: 76 -->
<!-- /react-text --><li data-reactid="77">所有特征向量都是垂直的，即彼此成直角</li><!-- react-text: 78 -->
<!-- /react-text --></ul><!-- react-text: 79 -->
<!-- /react-text --><h2 id="特征脸的计算（pca-方法）" data-reactid="80"><a href="#%E7%89%B9%E5%BE%81%E8%84%B8%E7%9A%84%E8%AE%A1%E7%AE%97%EF%BC%88pca-%E6%96%B9%E6%B3%95%EF%BC%89" aria-hidden="true" data-reactid="81"><span class="icon icon-link" data-reactid="82"></span></a><!-- react-text: 83 -->特征脸的计算（PCA 方法）<!-- /react-text --></h2><!-- react-text: 84 -->
<!-- /react-text --><p data-reactid="85"><!-- react-text: 86 -->在本节中，使用PCA确定特征脸的原始方案将会
被呈现。 在本文范围内描述的算法是一个变体。在PCA中可以找到一个详细的（以及更理论的）PCA的描述（Pissarenko，2002，第70-72页）。
1. 准备数据<!-- /react-text --><br data-reactid="87"/><!-- react-text: 88 -->
在这个步骤中，应准备有人脸组成的训练集（Γi）<!-- /react-text --></p><!-- react-text: 89 -->
<!-- /react-text --><ol start="2" data-reactid="90"><!-- react-text: 91 -->
<!-- /react-text --><li data-reactid="92"><!-- react-text: 93 -->
<!-- /react-text --><p data-reactid="94"><!-- react-text: 95 -->减去平均值<!-- /react-text --><br data-reactid="96"/><!-- react-text: 97 -->
应当计算出来平均矩阵Ψ，然后从Γi中减去，
并将结果存储在变量Φi中
<!-- /react-text --><img src="https://ooo.0o0.ooo/2017/04/30/590575d5edf57.jpg" width="287" height="142" data-reactid="98"/></p><!-- react-text: 99 -->
<!-- /react-text --></li><!-- react-text: 100 -->
<!-- /react-text --><li data-reactid="101"><!-- react-text: 102 -->
<!-- /react-text --><p data-reactid="103"><!-- react-text: 104 -->计算协方差矩阵<!-- /react-text --><br data-reactid="105"/><!-- react-text: 106 -->
计算协方差矩阵 C，依据如下
<!-- /react-text --><img src="https://ooo.0o0.ooo/2017/04/30/5905763b70744.jpg" width="279" height="97" data-reactid="107"/></p><!-- react-text: 108 -->
<!-- /react-text --></li><!-- react-text: 109 -->
<!-- /react-text --><li data-reactid="110"><!-- react-text: 111 -->
<!-- /react-text --><p data-reactid="112"><!-- react-text: 113 -->计算协方差的特征向量和特征值矩阵<!-- /react-text --><br data-reactid="114"/><!-- react-text: 115 -->
在这个步骤中，特征向量（特征脸）ui和对应的特征值λi应该计算。特征向量（特征面）必须被归一化才能使它们成为单位向量，即长度为1。描述确定的特征向量和特征值求法在此省略，因为它属于标准的数学程式。<!-- /react-text --></p><!-- react-text: 116 -->
<!-- /react-text --></li><!-- react-text: 117 -->
<!-- /react-text --><li data-reactid="118"><!-- react-text: 119 -->
<!-- /react-text --><p data-reactid="120"><!-- react-text: 121 -->选择主要组件<!-- /react-text --><br data-reactid="122"/><!-- react-text: 123 -->
从M个特征向量（特征脸）ui中，只应选择具有最高特征值的M0。特征值越高，特定特征向量描述的面的特征越多。可以省略具有低特征值的特征面，因为它们只解释了面部特征的一小部分。在确定M0特征脸ui之后，算法“训练”阶段结束。<!-- /react-text --></p><!-- react-text: 124 -->
<!-- /react-text --></li><!-- react-text: 125 -->
<!-- /react-text --></ol><!-- react-text: 126 -->
<!-- /react-text --><h2 id="改进原始算法" data-reactid="127"><a href="#%E6%94%B9%E8%BF%9B%E5%8E%9F%E5%A7%8B%E7%AE%97%E6%B3%95" aria-hidden="true" data-reactid="128"><span class="icon icon-link" data-reactid="129"></span></a><!-- react-text: 130 -->改进原始算法<!-- /react-text --></h2><!-- react-text: 131 -->
<!-- /react-text --><p data-reactid="132"><!-- react-text: 133 -->第5节描述的算法存在问题。协方差矩阵
在步骤3中（参见等式3）具有N 2×N 2的维数，因此将具有N 2
特征面和特征值。 对于256×256图像，这意味着必须计算65,536×65,536个矩阵，并计算65,536个特征面。在计算上，这并不是非常有效，因为大多数这些特征面对我们的任务没有用。所以，第三和第四步被Turk和Pentland（1991a）提出的方案所取代：
<!-- /react-text --><img src="https://ooo.0o0.ooo/2017/04/30/590579e61c7b3.jpg" width="387" height="209" data-reactid="134"/></p><!-- react-text: 135 -->
<!-- /react-text --><p data-reactid="136"><!-- react-text: 137 -->其中L是M×M矩阵，v是L的M个特征向量，u是特征面。 注意
使用公式C = AAT计算协方差矩阵C。只有为了解释A，才给出原始（低效）公式。这种方法的优点是必须只评估M数而不是N2。通常，仅作为主要成分（特征面）的M N2将是相关的。要执行的计算量从训练集（N）的数量减少到训练集（M）中的图像数量。<!-- /react-text --><br data-reactid="138"/><!-- react-text: 139 -->
在步骤5中，相关联的特征值允许根据它们的有用性对特征面进行排序。 通常，我们将仅使用M个特征面的一个子集，具有最大特征值的M0特征面。<!-- /react-text --></p><!-- react-text: 140 -->
<!-- /react-text --><h2 id="人脸分类" data-reactid="141"><a href="#%E4%BA%BA%E8%84%B8%E5%88%86%E7%B1%BB" aria-hidden="true" data-reactid="142"><span class="icon icon-link" data-reactid="143"></span></a><!-- react-text: 144 -->人脸分类<!-- /react-text --></h2><!-- react-text: 145 -->
<!-- /react-text --><p data-reactid="146"><!-- react-text: 147 -->新的人脸（未知）分类至已知人脸中需要2个步骤。<!-- /react-text --><br data-reactid="148"/><!-- react-text: 149 -->
首先，得到新人脸的特征脸，得到权重向量 ΩT(new)
<!-- /react-text --><img src="https://ooo.0o0.ooo/2017/04/30/59057d1dbbfeb.jpg" width="405" height="108" data-reactid="150"/></p><!-- react-text: 151 -->
<!-- /react-text --><p data-reactid="152">两个权重向量的欧几里得距离 d(Ωi, Ωj) 提供了一种衡量图像 i、j 的相似度的方法。如果ΩT(new)与其他人脸平均超过阈值θ，则认为其非人脸，或者构造一个新的脸的“簇”，使得类似的面部被分配给一个群集。</p><!-- react-text: 153 -->
<!-- /react-text --><h3 id="欧几里得距离" data-reactid="154"><a href="#%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB" aria-hidden="true" data-reactid="155"><span class="icon icon-link" data-reactid="156"></span></a><!-- react-text: 157 -->欧几里得距离<!-- /react-text --></h3><!-- react-text: 158 -->
<!-- /react-text --><p data-reactid="159"><!-- react-text: 160 -->x 为被特征向量描述的任意实例。
<!-- /react-text --><img src="https://ooo.0o0.ooo/2017/04/30/59057f7c67c5a.jpg" width="342" height="53" data-reactid="161"/><!-- react-text: 162 -->
其中ar(x) 表示实例 x 中的第 r 个属性值，那么两个实例xi,xj的欧几里得距离定义为：
<!-- /react-text --><img src="https://ooo.0o0.ooo/2017/04/30/59057ff879e7e.jpg" width="455" height="121" data-reactid="163"/></p><!-- react-text: 164 -->
<!-- /react-text --><h2 id="参考文献" data-reactid="165"><a href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE" aria-hidden="true" data-reactid="166"><span class="icon icon-link" data-reactid="167"></span></a><!-- react-text: 168 -->参考文献<!-- /react-text --></h2><!-- react-text: 169 -->
<!-- /react-text --><ul data-reactid="170"><!-- react-text: 171 -->
<!-- /react-text --><li data-reactid="172">T. M. Mitchell. Machine Learning. McGraw-Hill International Editions, 1997.</li><!-- react-text: 173 -->
<!-- /react-text --><li data-reactid="174">D. Pissarenko. Neural networks for financial time series prediction: Overview over
recent research. BSc thesis, 2002.</li><!-- react-text: 175 -->
<!-- /react-text --><li data-reactid="176"><!-- react-text: 177 -->L. I. Smith. A tutorial on principal components analysis, February 2002. URL
<!-- /react-text --><a href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf" data-reactid="178">http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf</a><!-- react-text: 179 -->. (URL accessed on November 27, 2002).<!-- /react-text --></li><!-- react-text: 180 -->
<!-- /react-text --><li data-reactid="181"><!-- react-text: 182 -->M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience,
3(1), 1991a. URL <!-- /react-text --><a href="http://www.cs.ucsb.edu/~mturk/Papers/jcn" data-reactid="183">http://www.cs.ucsb.edu/~mturk/Papers/jcn</a><!-- react-text: 184 -->.
pdf. (URL accessed on November 27, 2002).<!-- /react-text --></li><!-- react-text: 185 -->
<!-- /react-text --><li data-reactid="186">M. A. Turk and A. P. Pentland. Face recognition using eigenfaces. In Proc. of Computer
Vision and Pattern Recognition, pages 586–591. IEEE, June 1991b. URL http:
//www.cs.wisc.edu/~dyer/cs540/handouts/mturk-CVPR91.pdf. (URL accessed
on November 27, 2002).</li><!-- react-text: 187 -->
<!-- /react-text --></ul><!-- react-text: 188 -->
<!-- /react-text --></article></div><div class="gitment-container" data-reactid="189"></div><div class="paginator" data-reactid="190"><a title="毕业论文初稿" class="prev" href="/毕业论文初稿" data-reactid="191">Prev</a><a title="毕设周记三" class="next" href="/graduation-weekly-record-3" data-reactid="192">Next</a></div></div></main><footer data-reactid="193"><div class="copyright" data-reactid="194"><p data-reactid="195"><!-- react-text: 196 -->© 2017. Powered By <!-- /react-text --><a href="https://github.com/picidaejs/picidae" data-reactid="197">Picidae</a></p></div></footer></div>
</div>
<script src="/PICIDAE_COMMON.js"></script>
<script src="/PICIDAE_ENTRY.js"></script>
</body>
</html>